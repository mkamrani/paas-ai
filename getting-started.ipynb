{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaaS AI - Getting Started Guide\n",
    "\n",
    "This notebook walks you through setting up and using PaaS AI from scratch. We'll cover:\n",
    "\n",
    "1. **Environment Setup** - Installing dependencies and setting up the environment\n",
    "2. **Custom Profile Creation** - Creating and activating a custom configuration profile\n",
    "3. **Documentation Server** - Running the example PaaS documentation with npm\n",
    "4. **Interactive Chat** - Starting the AI chat interface\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.11 or 3.12\n",
    "- Poetry (for dependency management)\n",
    "- Node.js 18+ (for documentation server)\n",
    "- OpenAI API key (for AI functionality)\n",
    "\n",
    "Let's get started! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "First, let's check our current directory and install the project dependencies using Poetry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/mohsen/Documents/me/learning/paas-ai\n",
      "‚úÖ Found pyproject.toml - we're in the right place!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Check current directory\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# Verify we're in the paas-ai project root\n",
    "if not (current_dir / \"pyproject.toml\").exists():\n",
    "    print(\"‚ùå Not in paas-ai project root. Please navigate to the project directory.\")\n",
    "else:\n",
    "    print(\"‚úÖ Found pyproject.toml - we're in the right place!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing all dependencies...\n",
      "‚úÖ All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install project dependencies using Make\n",
    "print(\"üì¶ Installing all dependencies...\")\n",
    "result = subprocess.run([\"make\", \"install-all\"], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ All dependencies installed successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Installation failed: {result.stderr}\")\n",
    "    print(\"Please run 'make install-all' manually\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Variables Setup\n",
    "\n",
    "Let's set up the required environment variables. You'll need an OpenAI API key for the AI functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ .env file already exists\n",
      "\n",
      "‚ö†Ô∏è  Please edit the .env file and add your OpenAI API key:\n",
      "   OPENAI_API_KEY=your_actual_api_key_here\n"
     ]
    }
   ],
   "source": [
    "# Check if .env file exists, if not create from example\n",
    "env_file = Path(\".env\")\n",
    "env_example = Path(\"env.example\")\n",
    "\n",
    "if not env_file.exists() and env_example.exists():\n",
    "    # Copy env.example to .env\n",
    "    with open(env_example, 'r') as src, open(env_file, 'w') as dst:\n",
    "        dst.write(src.read())\n",
    "    print(\"‚úÖ Created .env file from env.example\")\n",
    "elif env_file.exists():\n",
    "    print(\"‚úÖ .env file already exists\")\n",
    "else:\n",
    "    print(\"‚ùå No env.example found. Creating basic .env file...\")\n",
    "    with open(env_file, 'w') as f:\n",
    "        f.write(\"OPENAI_API_KEY=your_openai_api_key_here\\n\")\n",
    "    print(\"‚úÖ Created basic .env file\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Please edit the .env file and add your OpenAI API key:\")\n",
    "print(\"   OPENAI_API_KEY=your_actual_api_key_here\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and Activate Custom Profile\n",
    "\n",
    "Now let's create a custom configuration profile. This allows us to customize the AI behavior, embedding models, and other settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No config.yaml found. Let's create one...\n",
      "‚úÖ Config initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check current config.yaml\n",
    "config_file = Path.home() / \".paas-ai\" / \"config.yaml\"\n",
    "\n",
    "if config_file.exists():\n",
    "    print(\"‚úÖ Found existing config.yaml\")\n",
    "    \n",
    "    # Read and display current profile\n",
    "    with open(config_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Extract current profile\n",
    "    import yaml\n",
    "    config = yaml.safe_load(content)\n",
    "    current_profile = config.get('current', 'default')\n",
    "    \n",
    "    print(f\"üìã Current active profile: {current_profile}\")\n",
    "    print(f\"üìã Available profiles: {list(config.get('profiles', {}).keys())}\")\n",
    "else:\n",
    "    print(\"‚ùå No config.yaml found. Let's create one...\")\n",
    "    \n",
    "    # Initialize config\n",
    "    result = subprocess.run([\"poetry\", \"run\", \"paas-ai\", \"config\", \"init\"], \n",
    "                           capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Config initialized successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Config initialization failed: {result.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating custom profile: notebook_demo\n",
      "‚úÖ Profile 'notebook_demo' created successfully!\n",
      "‚ÑπÔ∏è 17:15:24 INFO     Registered 7 default tools\n",
      "‚ÑπÔ∏è 17:15:24 INFO     Token callback system initialized with 3 built-in callbacks\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI] Setting 'notebook_demo' as active profile (was 'my-custom-profile')\n",
      "‚úÖ 17:15:24 SUCCESS  [CLI] Profile 'notebook_demo' created successfully\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI] Configuration saved to: /Users/mohsen/.paas-ai/config.yaml\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI] \n",
      "Profile 'notebook_demo' details:\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI]   Embedding: sentence_transformers (all-MiniLM-L6-v2)\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI]   Vector Store: chroma\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI]   Persist Directory: ./rag_data/notebook_demo\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI]   Collection: notebook_demo_knowledge\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI]   Retriever: similarity\n",
      "‚ÑπÔ∏è 17:15:24 INFO     [CLI]   Batch Size: 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new custom profile called \"notebook_demo\"\n",
    "profile_name = \"notebook_demo\"\n",
    "\n",
    "print(f\"üîß Creating custom profile: {profile_name}\")\n",
    "\n",
    "# Create profile using CLI (correct command is \"add-profile\")\n",
    "result = subprocess.run([\n",
    "    \"poetry\", \"run\", \"paas-ai\", \"config\", \"add-profile\", profile_name,\n",
    "    \"--embedding-type\", \"sentence_transformers\",\n",
    "    \"--embedding-model\", \"all-MiniLM-L6-v2\",\n",
    "    \"--vectorstore-type\", \"chroma\",\n",
    "    \"--activate\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Profile '{profile_name}' created successfully!\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(f\"‚ùå Profile creation failed: {result.stderr}\")\n",
    "    print(\"You can create it manually or use an existing profile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Setting profile as current: notebook_demo\n",
      "‚úÖ Profile 'notebook_demo' set as current successfully!\n",
      "‚ÑπÔ∏è 17:15:40 INFO     Registered 7 default tools\n",
      "‚ÑπÔ∏è 17:15:40 INFO     Token callback system initialized with 3 built-in callbacks\n",
      "‚úÖ 17:15:40 SUCCESS  [CLI] Active profile changed from 'notebook_demo' to 'notebook_demo'\n",
      "‚ÑπÔ∏è 17:15:40 INFO     [CLI] Configuration saved to: /Users/mohsen/.paas-ai/config.yaml\n",
      "‚ÑπÔ∏è 17:15:40 INFO     [CLI] \n",
      "Profile 'notebook_demo': Custom profile: notebook_demo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Activate the custom profile (if not already activated by --activate flag)\n",
    "print(f\"üîÑ Setting profile as current: {profile_name}\")\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"poetry\", \"run\", \"paas-ai\", \"config\", \"set-current\", profile_name\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Profile '{profile_name}' set as current successfully!\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(f\"‚ùå Profile activation failed: {result.stderr}\")\n",
    "    print(\"The profile may already be active from the --activate flag above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Example PaaS Documentation\n",
    "\n",
    "Now let's start the documentation server. This provides example PaaS configurations and documentation that the AI can reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Setting up documentation server...\n"
     ]
    }
   ],
   "source": [
    "# Navigate to the docs directory and install dependencies\n",
    "docs_dir = Path(\"examples/paas/docs\")\n",
    "print(\"üì¶ Setting up documentation server...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install npm dependencies and start server\n",
    "original_dir = os.getcwd()\n",
    "os.chdir(docs_dir)\n",
    "\n",
    "# Install dependencies\n",
    "subprocess.run([\"npm\", \"install\"], capture_output=True, text=True)\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "\n",
    "# Return to project root\n",
    "os.chdir(original_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> cool-demo-paas@0.0.0 start\n",
      "> docusaurus start\n",
      "\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[33m ------------------------------------------------------------------------------ \u001b[39m\u001b[33m \u001b[39m                                                                              \u001b[33m \u001b[39m\u001b[33m \u001b[39m                        Update available \u001b[2m3.8.1\u001b[22m ‚Üí \u001b[32m3.9.1\u001b[39m                        \u001b[33m \u001b[39m\u001b[33m \u001b[39m                                                                              \u001b[33m \u001b[39m\u001b[33m \u001b[39m       To upgrade Docusaurus packages with the latest version, run the        \u001b[33m \u001b[39m\u001b[33m \u001b[39m                             following command:                               \u001b[33m \u001b[39m\u001b[33m \u001b[39m       \u001b[36m`npm i @docusaurus/core@latest @docusaurus/preset-classic@latest\u001b[39m       \u001b[33m \u001b[39m\u001b[33m \u001b[39m      \u001b[36m@docusaurus/module-type-aliases@latest @docusaurus/tsconfig@latest\u001b[39m      \u001b[33m \u001b[39m\u001b[33m \u001b[39m                          \u001b[36m@docusaurus/types@latest`\u001b[39m                           \u001b[33m \u001b[39m\u001b[33m \u001b[39m                                                                              \u001b[33m \u001b[39m\u001b[33m ------------------------------------------------------------------------------ \u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[36m\u001b[1m[INFO]\u001b[22m\u001b[39m Starting the development server...\n",
      "\u001b[32m\u001b[1m[SUCCESS]\u001b[22m\u001b[39m Docusaurus website is running at: \u001b[36m\u001b[4mhttp://localhost:3000/\u001b[24m\u001b[39m\n",
      "\u001b[90m[\u001b[90mwebpackbar\u001b[90m]\u001b[39m \u001b[36m‚Ñπ\u001b[39m Compiling Client\n",
      "\u001b[90m[\u001b[90mwebpackbar\u001b[90m]\u001b[39m \u001b[32m‚úî\u001b[39m Client: Compiled successfully in 443.57ms\n",
      "\u001b[1mclient\u001b[39m\u001b[22m (webpack 5.101.3) compiled \u001b[1m\u001b[32msuccessfully\u001b[39m\u001b[22m\n",
      "üöÄ Documentation server started at http://localhost:3000\n"
     ]
    }
   ],
   "source": [
    "# Start the documentation server in background\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def start_docs_server():\n",
    "    os.chdir(docs_dir)\n",
    "    subprocess.run([\"npm\", \"start\"], capture_output=False)\n",
    "\n",
    "server_thread = threading.Thread(target=start_docs_server, daemon=True)\n",
    "server_thread.start()\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"üöÄ Documentation server started at http://localhost:3000\")\n",
    "os.chdir(original_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: Clean Up Previous Data (Optional)\n",
    "\n",
    "Before loading new knowledge, you can optionally clean up any existing RAG data or conversation history from previous runs. This ensures a fresh start and prevents conflicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to clean up existing RAG data and conversation history? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up existing data...\n",
      "üìù No existing RAG data directory found\n",
      "üìù No existing conversation history found\n",
      "‚úÖ Cleanup completed! Starting with fresh data.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Clean up existing data for a fresh start\n",
    "import shutil\n",
    "\n",
    "# Define paths we'll need\n",
    "profile_rag_dir = Path(f\"rag_data/{profile_name}\")\n",
    "\n",
    "cleanup_choice = input(\"Would you like to clean up existing RAG data and conversation history? (y/N): \")\n",
    "\n",
    "if cleanup_choice.lower() in ['y', 'yes']:\n",
    "    print(\"üßπ Cleaning up existing data...\")\n",
    "    \n",
    "    # Clean up RAG data directory\n",
    "    rag_data_dir = Path(\"rag_data\")\n",
    "    if rag_data_dir.exists():\n",
    "        try:\n",
    "            shutil.rmtree(rag_data_dir)\n",
    "            print(\"‚úÖ Removed existing RAG data directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not remove RAG data directory: {e}\")\n",
    "    else:\n",
    "        print(\"üìù No existing RAG data directory found\")\n",
    "    \n",
    "    # Clean up conversation history (only the conversations.db file, NOT the config!)\n",
    "    conversations_db = Path.home() / \".paas-ai\" / \"conversations.db\"\n",
    "    if conversations_db.exists():\n",
    "        try:\n",
    "            conversations_db.unlink()\n",
    "            print(\"‚úÖ Removed existing conversation history\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not remove conversation history: {e}\")\n",
    "    else:\n",
    "        print(\"üìù No existing conversation history found\")\n",
    "    \n",
    "    print(\"‚úÖ Cleanup completed! Starting with fresh data.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping cleanup - keeping existing data\")\n",
    "    print(\"üí° If you encounter issues, you can run cleanup manually later\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Knowledge Base (Optional)\n",
    "\n",
    "Before starting the chat, let's optionally load some knowledge into the RAG system. This step is optional but recommended for better AI responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù No knowledge base found for this profile\n",
      "üí° We'll load the data in the next step\n"
     ]
    }
   ],
   "source": [
    "# Check if we have any existing knowledge base (profile-specific path)\n",
    "profile_rag_dir = Path(f\"rag_data/{profile_name}\")\n",
    "\n",
    "if profile_rag_dir.exists():\n",
    "    print(f\"‚úÖ Found profile-specific RAG data: {profile_rag_dir}\")\n",
    "    \n",
    "    # Check for ChromaDB files\n",
    "    chroma_db = profile_rag_dir / \"chroma.sqlite3\"\n",
    "    if chroma_db.exists():\n",
    "        print(\"‚úÖ Knowledge base is set up!\")\n",
    "    else:\n",
    "        print(\"üìù RAG directory exists but no data found\")\n",
    "else:\n",
    "    print(\"üìù No knowledge base found for this profile\")\n",
    "    print(\"üí° We'll load the data in the next step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to load the PaaS documentation into the knowledge base? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading PaaS documentation from CSV...\n",
      "‚è≥ This may take a few minutes...\n",
      "üîÑ Running command with live output:\n",
      "poetry run paas-ai rag resources add-batch --csv-file examples/paas/paas_docs_urls.csv\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è 17:17:35 INFO     Registered 7 default tools\n",
      "‚ÑπÔ∏è 17:17:35 INFO     Token callback system initialized with 3 built-in callbacks\n",
      "‚ÑπÔ∏è 17:17:35 INFO     [CLI] Using configuration profile with EmbeddingType.SENTENCE_TRANSFORMERS embeddings\n",
      "‚ÑπÔ∏è 17:17:35 INFO     [CLI] Found 11 resources in CSV file\n",
      "‚ÑπÔ∏è 17:17:39 INFO     [CLI] Citation system enabled with verbosity: standard\n",
      "‚ÑπÔ∏è 17:17:39 INFO     [CLI] Adding 11 resources to knowledge base\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/intro\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:17:39 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/intro\n",
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:17:40 INFO     Loading URL: http://localhost:3000/intro\n",
      "[FETCH]... ‚Üì http://localhost:3000/intro                                        \n",
      "| ‚úì | ‚è±: 4.04s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/intro                                        \n",
      "| ‚úì | ‚è±: 0.01s \n",
      "[COMPLETE] ‚óè http://localhost:3000/intro                                        \n",
      "| ‚úì | ‚è±: 4.05s \n",
      "‚ÑπÔ∏è 17:17:44 INFO     Successfully loaded http://localhost:3000/intro - 4923 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 5.11s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 8 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 8 ‚Üí 8 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:17:45 INFO     [CLI] Created new vectorstore\n",
      "Stage 'vectorstore' completed: 8 ‚Üí 8 documents in 0.70s\n",
      "Pipeline completed: 8 documents processed in 5.81s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/networking\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:17:45 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/networking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:17:45 INFO     Loading URL: http://localhost:3000/dsl/networking\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/networking                               \n",
      "| ‚úì | ‚è±: 3.92s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/networking                               \n",
      "| ‚úì | ‚è±: 0.04s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/networking                               \n",
      "| ‚úì | ‚è±: 3.96s \n",
      "‚ÑπÔ∏è 17:17:49 INFO     Successfully loaded http://localhost:3000/dsl/networking - 15631 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.45s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 23 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 23 ‚Üí 23 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:17:49 INFO     [CLI] Added 23 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 23 ‚Üí 23 documents in 0.16s\n",
      "Pipeline completed: 23 documents processed in 4.61s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/ec2\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:17:49 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/ec2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:17:50 INFO     Loading URL: http://localhost:3000/dsl/ec2\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/ec2                                      \n",
      "| ‚úì | ‚è±: 3.89s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/ec2                                      \n",
      "| ‚úì | ‚è±: 0.02s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/ec2                                      \n",
      "| ‚úì | ‚è±: 3.92s \n",
      "‚ÑπÔ∏è 17:17:54 INFO     Successfully loaded http://localhost:3000/dsl/ec2 - 6364 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.37s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 9 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 9 ‚Üí 9 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:17:54 INFO     [CLI] Added 9 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 9 ‚Üí 9 documents in 0.13s\n",
      "Pipeline completed: 9 documents processed in 4.49s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/ecs\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:17:54 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/ecs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:17:54 INFO     Loading URL: http://localhost:3000/dsl/ecs\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/ecs                                      \n",
      "| ‚úì | ‚è±: 3.94s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/ecs                                      \n",
      "| ‚úì | ‚è±: 0.03s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/ecs                                      \n",
      "| ‚úì | ‚è±: 3.97s \n",
      "‚ÑπÔ∏è 17:17:58 INFO     Successfully loaded http://localhost:3000/dsl/ecs - 8922 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.41s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 12 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 12 ‚Üí 12 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:17:58 INFO     [CLI] Added 12 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 12 ‚Üí 12 documents in 0.10s\n",
      "Pipeline completed: 12 documents processed in 4.51s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/alb\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:17:58 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/alb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:17:59 INFO     Loading URL: http://localhost:3000/dsl/alb\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/alb                                      \n",
      "| ‚úì | ‚è±: 3.97s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/alb                                      \n",
      "| ‚úì | ‚è±: 0.04s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/alb                                      \n",
      "| ‚úì | ‚è±: 4.00s \n",
      "‚ÑπÔ∏è 17:18:03 INFO     Successfully loaded http://localhost:3000/dsl/alb - 8566 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.44s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 14 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 14 ‚Üí 14 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:03 INFO     [CLI] Added 14 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 14 ‚Üí 14 documents in 0.11s\n",
      "Pipeline completed: 14 documents processed in 4.54s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/rds\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:18:03 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/rds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:18:03 INFO     Loading URL: http://localhost:3000/dsl/rds\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/rds                                      \n",
      "| ‚úì | ‚è±: 3.95s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/rds                                      \n",
      "| ‚úì | ‚è±: 0.03s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/rds                                      \n",
      "| ‚úì | ‚è±: 3.98s \n",
      "‚ÑπÔ∏è 17:18:07 INFO     Successfully loaded http://localhost:3000/dsl/rds - 9674 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.43s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 14 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 14 ‚Üí 14 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:08 INFO     [CLI] Added 14 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 14 ‚Üí 14 documents in 0.10s\n",
      "Pipeline completed: 14 documents processed in 4.53s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/route53\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:18:08 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/route53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:18:08 INFO     Loading URL: http://localhost:3000/dsl/route53\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/route53                                  \n",
      "| ‚úì | ‚è±: 3.97s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/route53                                  \n",
      "| ‚úì | ‚è±: 0.03s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/route53                                  \n",
      "| ‚úì | ‚è±: 4.01s \n",
      "‚ÑπÔ∏è 17:18:12 INFO     Successfully loaded http://localhost:3000/dsl/route53 - 8737 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.44s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 13 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 13 ‚Üí 13 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:12 INFO     [CLI] Added 13 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 13 ‚Üí 13 documents in 0.10s\n",
      "Pipeline completed: 13 documents processed in 4.55s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/dsl/acm\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:18:12 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/dsl/acm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:18:12 INFO     Loading URL: http://localhost:3000/dsl/acm\n",
      "[FETCH]... ‚Üì http://localhost:3000/dsl/acm                                      \n",
      "| ‚úì | ‚è±: 3.93s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/dsl/acm                                      \n",
      "| ‚úì | ‚è±: 0.03s \n",
      "[COMPLETE] ‚óè http://localhost:3000/dsl/acm                                      \n",
      "| ‚úì | ‚è±: 3.96s \n",
      "‚ÑπÔ∏è 17:18:16 INFO     Successfully loaded http://localhost:3000/dsl/acm - 8664 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.41s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 13 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 13 ‚Üí 13 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:17 INFO     [CLI] Added 13 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 13 ‚Üí 13 documents in 0.10s\n",
      "Pipeline completed: 13 documents processed in 4.51s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/guidelines/best-practices\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:18:17 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/guidelines/best-practices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:18:17 INFO     Loading URL: http://localhost:3000/guidelines/best-practices\n",
      "[FETCH]... ‚Üì http://localhost:3000/guidelines/best-practices                    \n",
      "| ‚úì | ‚è±: 3.90s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/guidelines/best-practices                    \n",
      "| ‚úì | ‚è±: 0.04s \n",
      "[COMPLETE] ‚óè http://localhost:3000/guidelines/best-practices                    \n",
      "| ‚úì | ‚è±: 3.95s \n",
      "‚ÑπÔ∏è 17:18:21 INFO     Successfully loaded http://localhost:3000/guidelines/best-practices - 14866 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.38s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 19 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 19 ‚Üí 19 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:21 INFO     [CLI] Added 19 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 19 ‚Üí 19 documents in 0.12s\n",
      "Pipeline completed: 19 documents processed in 4.51s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/guidelines/how-tos\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:18:21 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/guidelines/how-tos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:18:22 INFO     Loading URL: http://localhost:3000/guidelines/how-tos\n",
      "[FETCH]... ‚Üì http://localhost:3000/guidelines/how-tos                           \n",
      "| ‚úì | ‚è±: 3.89s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/guidelines/how-tos                           \n",
      "| ‚úì | ‚è±: 0.05s \n",
      "[COMPLETE] ‚óè http://localhost:3000/guidelines/how-tos                           \n",
      "| ‚úì | ‚è±: 3.95s \n",
      "‚ÑπÔ∏è 17:18:26 INFO     Successfully loaded http://localhost:3000/guidelines/how-tos - 15946 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.51s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 22 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 22 ‚Üí 22 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:26 INFO     [CLI] Added 22 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 22 ‚Üí 22 documents in 0.12s\n",
      "Pipeline completed: 22 documents processed in 4.63s\n",
      "Starting pipeline 'load_to_validate_to_split_to_enrich_to_vectorstore' for resource: http://localhost:3000/guidelines/networking\n",
      "Executing stage: load\n",
      "‚ÑπÔ∏è 17:18:26 INFO     Validated Crawl4AI web loader config for URL: http://localhost:3000/guidelines/networking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.7.4 \n",
      "‚ÑπÔ∏è 17:18:26 INFO     Loading URL: http://localhost:3000/guidelines/networking\n",
      "[FETCH]... ‚Üì http://localhost:3000/guidelines/networking                        \n",
      "| ‚úì | ‚è±: 3.88s \n",
      "[SCRAPE].. ‚óÜ http://localhost:3000/guidelines/networking                        \n",
      "| ‚úì | ‚è±: 0.04s \n",
      "[COMPLETE] ‚óè http://localhost:3000/guidelines/networking                        \n",
      "| ‚úì | ‚è±: 3.93s \n",
      "‚ÑπÔ∏è 17:18:30 INFO     Successfully loaded http://localhost:3000/guidelines/networking - 15275 characters\n",
      "Stage 'load' completed: 0 ‚Üí 1 documents in 4.37s\n",
      "Executing stage: validate\n",
      "Stage 'validate' completed: 1 ‚Üí 1 documents in 0.00s\n",
      "Executing stage: split\n",
      "Stage 'split' completed: 1 ‚Üí 22 documents in 0.00s\n",
      "Executing stage: enrich\n",
      "Stage 'enrich' completed: 22 ‚Üí 22 documents in 0.00s\n",
      "Executing stage: vectorstore\n",
      "‚ÑπÔ∏è 17:18:30 INFO     [CLI] Added 22 documents to existing vectorstore\n",
      "Stage 'vectorstore' completed: 22 ‚Üí 22 documents in 0.13s\n",
      "Pipeline completed: 22 documents processed in 4.50s\n",
      "‚úÖ 17:18:30 SUCCESS  [CLI] Completed processing. Successful: 11, Failed: 0, Total documents: 169\n",
      "‚úÖ 17:18:30 SUCCESS  [CLI] Batch processing completed:\n",
      "‚ÑπÔ∏è 17:18:30 INFO     [CLI]   Successful: 11\n",
      "‚ÑπÔ∏è 17:18:30 INFO     [CLI]   Failed: 0\n",
      "‚ÑπÔ∏è 17:18:30 INFO     [CLI]   Total documents: 169\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Documentation loaded successfully!\n",
      "‚úÖ Verified: Knowledge base created at rag_data/notebook_demo\n"
     ]
    }
   ],
   "source": [
    "# Load PaaS documentation using the CSV file\n",
    "load_knowledge = input(\"Would you like to load the PaaS documentation into the knowledge base? (y/N): \")\n",
    "\n",
    "if load_knowledge.lower() in ['y', 'yes']:\n",
    "    print(\"üìö Loading PaaS documentation from CSV...\")\n",
    "    print(\"‚è≥ This may take a few minutes...\")\n",
    "    \n",
    "    # Load documentation using the CSV file (with real-time output)\n",
    "    print(\"üîÑ Running command with live output:\")\n",
    "    print(\"poetry run paas-ai rag resources add-batch --csv-file examples/paas/paas_docs_urls.csv\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = subprocess.run([\n",
    "        \"poetry\", \"run\", \"paas-ai\", \"rag\", \"resources\", \"add-batch\",\n",
    "        \"--csv-file\", \"examples/paas/paas_docs_urls.csv\"\n",
    "    ], text=True)  # Remove capture_output to see live output\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Documentation loaded successfully!\")\n",
    "        # Check if data was actually loaded\n",
    "        if profile_rag_dir.exists() and (profile_rag_dir / \"chroma.sqlite3\").exists():\n",
    "            print(f\"‚úÖ Verified: Knowledge base created at {profile_rag_dir}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Warning: Command succeeded but no data files found\")\n",
    "    else:\n",
    "        print(f\"‚ùå Documentation loading failed with return code: {result.returncode}\")\n",
    "        print(\"üí° You can try loading it manually later\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping knowledge base loading\")\n",
    "    print(\"üí° You can load it later using: poetry run paas-ai rag resources add-batch --csv-file examples/paas/paas_docs_urls.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Interactive Chat\n",
    "\n",
    "Now for the exciting part - let's start the interactive chat with the AI! This will launch the PaaS AI chat interface where you can ask questions about PaaS configurations, get help with deployments, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Everything is set up and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Ready to start chat!\n",
    "print(\"ü§ñ Everything is set up and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Ready to start PaaS AI Chat!\n",
      "\n",
      "============================================================\n",
      "üöÄ PAAS AI INTERACTIVE CHAT\n",
      "============================================================\n",
      "\n",
      "üí° What you can do:\n",
      "  ‚Ä¢ Ask questions about PaaS configurations\n",
      "  ‚Ä¢ Get help with Kubernetes, Docker, Terraform\n",
      "  ‚Ä¢ Request example configurations\n",
      "  ‚Ä¢ Get best practices and guidelines\n",
      "\n",
      "üéÆ Special commands:\n",
      "  ‚Ä¢ 'tools' - Show available agent tools\n",
      "  ‚Ä¢ 'config' - Show current configuration\n",
      "  ‚Ä¢ 'tokens' - Show token usage (if tracking enabled)\n",
      "  ‚Ä¢ 'exit' or 'quit' - End the session\n",
      "\n",
      "üìö Resources available:\n",
      "  ‚Ä¢ Documentation server: http://localhost:3000\n",
      "  ‚Ä¢ Local knowledge base (if loaded)\n",
      "  ‚Ä¢ Built-in PaaS expertise\n",
      "============================================================\n",
      "\n",
      "üéØ Example questions to try:\n",
      "  ‚Ä¢ 'Create a simple web application deployment'\n",
      "  ‚Ä¢ 'What are the best practices for microservices?'\n",
      "  ‚Ä¢ 'Show me a Kubernetes deployment example'\n",
      "  ‚Ä¢ 'How do I set up load balancing?'\n",
      "\n",
      "‚ö° Starting chat in 3 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Display chat startup information\n",
    "print(\"ü§ñ Ready to start PaaS AI Chat!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PAAS AI INTERACTIVE CHAT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° What you can do:\")\n",
    "print(\"  ‚Ä¢ Ask questions about PaaS configurations\")\n",
    "print(\"  ‚Ä¢ Get help with Kubernetes, Docker, Terraform\")\n",
    "print(\"  ‚Ä¢ Request example configurations\")\n",
    "print(\"  ‚Ä¢ Get best practices and guidelines\")\n",
    "print(\"\\nüéÆ Special commands:\")\n",
    "print(\"  ‚Ä¢ 'tools' - Show available agent tools\")\n",
    "print(\"  ‚Ä¢ 'config' - Show current configuration\")\n",
    "print(\"  ‚Ä¢ 'tokens' - Show token usage (if tracking enabled)\")\n",
    "print(\"  ‚Ä¢ 'exit' or 'quit' - End the session\")\n",
    "print(\"\\nüìö Resources available:\")\n",
    "print(\"  ‚Ä¢ Documentation server: http://localhost:3000\")\n",
    "print(\"  ‚Ä¢ Local knowledge base (if loaded)\")\n",
    "print(\"  ‚Ä¢ Built-in PaaS expertise\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ Example questions to try:\")\n",
    "print(\"  ‚Ä¢ 'Create a simple web application deployment'\")\n",
    "print(\"  ‚Ä¢ 'What are the best practices for microservices?'\")\n",
    "print(\"  ‚Ä¢ 'Show me a Kubernetes deployment example'\")\n",
    "print(\"  ‚Ä¢ 'How do I set up load balancing?'\")\n",
    "\n",
    "print(\"\\n‚ö° Starting chat in 3 seconds...\")\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TO START CHAT, RUN THIS COMMAND IN YOUR TERMINAL:\n",
      "============================================================\n",
      "\n",
      "poetry run paas-ai agent chat --show-config\n",
      "\n",
      "üí° Or with your custom profile:\n",
      "poetry run paas-ai agent chat --config-profile notebook_demo --show-config\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Instructions to start the chat\n",
    "print(\"üéØ TO START CHAT, RUN THIS COMMAND IN YOUR TERMINAL:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\npoetry run paas-ai agent chat --show-config\")\n",
    "print(\"\\nüí° Or with your custom profile:\")\n",
    "print(f\"poetry run paas-ai agent chat --config-profile {profile_name} --show-config\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Setup Complete!\n",
    "\n",
    "Congratulations! You've successfully set up PaaS AI. Here's what we accomplished:\n",
    "\n",
    "### ‚úÖ What's Running:\n",
    "1. **Dependencies Installed** - All Python packages via Poetry\n",
    "2. **Custom Profile Created** - `notebook_demo` profile configured\n",
    "3. **Documentation Server** - Running on http://localhost:3000\n",
    "4. **Knowledge Base** - Ready for queries (loaded if you chose to)\n",
    "5. **Chat Interface** - Ready to start\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Open a terminal** and navigate to this project directory\n",
    "2. **Run the chat command**:\n",
    "   ```bash\n",
    "   poetry run paas-ai agent chat --show-config\n",
    "   ```\n",
    "3. **Start asking questions** about PaaS configurations!\n",
    "\n",
    "### üí° Tips:\n",
    "- Visit http://localhost:3000 to browse the documentation\n",
    "- Use `poetry run paas-ai --help` to see all available commands\n",
    "- Check `poetry run paas-ai config show` to see your current configuration\n",
    "- Load more knowledge with `poetry run paas-ai rag index --help`\n",
    "\n",
    "### üõ†Ô∏è Troubleshooting:\n",
    "- If chat doesn't start, check your OpenAI API key in `.env`\n",
    "- If documentation server isn't running, restart with `npm start` in `examples/paas/docs/`\n",
    "- For configuration issues, try `poetry run paas-ai config init`\n",
    "\n",
    "Happy coding with PaaS AI! ü§ñ‚ú®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup (Optional)\n",
    "\n",
    "When you're done experimenting, you can clean up resources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup commands\n",
    "print(\"üßπ Cleanup options:\")\n",
    "print(\"\\n1. Stop documentation server:\")\n",
    "print(\"   - Press Ctrl+C in the terminal where npm start is running\")\n",
    "print(\"   - Or find and kill the process: pkill -f 'docusaurus start'\")\n",
    "\n",
    "print(\"\\n2. Remove generated data (if you want to start fresh):\")\n",
    "print(\"   - rm -rf rag_data/  # Removes knowledge base\")\n",
    "print(\"   - rm -rf ~/.paas-ai/  # Removes conversation history\")\n",
    "\n",
    "print(\"\\n3. Reset configuration:\")\n",
    "print(\"   - poetry run paas-ai config init  # Reinitialize config\")\n",
    "\n",
    "print(\"\\nüí° You don't need to run cleanup unless you want to start over!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
